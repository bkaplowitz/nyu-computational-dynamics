{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2c8ede",
   "metadata": {},
   "source": [
    "# Q-learning in the worker search model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2816e31",
   "metadata": {},
   "source": [
    "In this problem, we implement the Q-learning algorithm in the context of the McCall (1970) model. The problem is based on the corresponding <a href=\"https://python.quantecon.org/mccall_q.html\">QuantEcon lecture</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c001d",
   "metadata": {},
   "source": [
    "\n",
    "## Worker search model\n",
    "\n",
    "The worker lives in an infinite-horizon economy, with discrete time \\thinspace $t=0,1,2,\\ldots $. Every period $t$, an iid wage offer $w$ from distribution $F\\left(w\\right) $ is drawn, with $F\\left( 0\\right) =0$, $F\\left( B\\right) =1$ for some $B>0$.\n",
    "\n",
    "The worker decides to {accept or reject} the offer, $a_{t}\\in \\left\\{ \\text{accept, reject}\\right\\} $. Acceptance means that the worker receives income $y_{t}=w$ forever. Rejection implies that  the worker receives unemployment benefit $y_{t}=c$ and moves to next period where a new offer is drawn. Time is discounted at rate $\\beta \\in \\lbrack 0,1)$.\n",
    "\n",
    "The worker solves the sequence problem\n",
    "\n",
    "\\begin{equation}\n",
    "V_{0}^{\\ast }=\\max_{\\left\\{ a_{t}\\right\\} _{t=0}^{\\infty }}E_{0}\\left[\n",
    "\\sum_{t=0}^{\\infty }\\beta ^{t}y_{t}\\right]\n",
    "\\label{eq:mccall_infinite_horizon}\n",
    "\\end{equation}\n",
    "\n",
    "where $a_{t}\\in \\left\\{ \\text{accept, reject}\\right\\} $ if the worker has\n",
    "not yet accepted any earlier offer, and $a_{t}\\in \\left\\{ {}\\right\\} $\n",
    "otherwise. $V_{0}^{\\ast }$ denotes the {value function}, and we assume that $V_{0}^{\\ast }$\n",
    "conditions on the initial offer $w_{0}$ being observed.\n",
    "\n",
    "Every decision $a_{t}$ is made conditional on the time-$t$ information\n",
    "set, which contains the history of all offers up to time $t$, $w^{t}=\\left(\n",
    "w_{0},\\ldots ,w_{t}\\right) $. $E\\left[ \\cdot \\right] $ is the mathematical expectations operator\n",
    "\n",
    "\\begin{equation*}\n",
    "E\\left[ w\\right] =\\int_{0}^{B}wdF\\left( w\\right) =\\int_{0}^{B}wf\\left(\n",
    "w\\right) dw.\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "The of a worker with current offer $w$ at hand can\n",
    "be formulated recursively as\n",
    "\n",
    "\\begin{equation*}\n",
    "V\\left( w\\right) =\\max_{\\left\\{ \\text{accept, reject}\\right\\} }\\left\\{\n",
    "V^{a}\\left( w\\right) ,c+\\beta \\int_{0}^{B}V\\left( w^{\\prime }\\right)\n",
    "dF\\left( w^{\\prime }\\right) \\right\\}\n",
    "\\end{equation*}\n",
    "\n",
    "where $V^{a}\\left( w\\right) $ is the value of accepting the offer.\n",
    "\n",
    "In this formulation, we <b>assumed</b> that once an offer $w$ is accepted, the worker works at that wage\n",
    "forever\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{a}\\left( w\\right) =\\frac{w}{1-\\beta }.\n",
    "\\end{equation*}\n",
    "\n",
    "However, as it turns out, this assumption can be dropped without any consequence, and the problem modified as follows. In every period, the worker can decide whether to continue working at the same wage $w$ (i.e., accept the same wage offer $w$ again for the next period), or leave to unemployent, in which case a new wage offer arrives in the next period.\n",
    "\n",
    "The value of accepting an offer can then be written as follows\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{a}\\left( w\\right) =w+\\beta \\max_{\\left\\{ \\text{accept, reject}\\right\\}\n",
    "}\\left\\{ V^{a}\\left( w\\right) ,c+\\beta \\int_{0}^{B}V\\left( w^{\\prime\n",
    "}\\right) dF\\left( w^{\\prime }\\right) \\right\\}\n",
    "\\end{equation*}\n",
    "\n",
    "In this iid environment, even if the worker is allowed to leave the current job that guarantees wage $w$ which was previously accepted, such an option would never be exercised. In other words, if the wage was sufficiently high to be accepted in one period, it is sufficiently high to be accepted in any other period.\n",
    "\n",
    "Notice that this conclusion would be different in a model with an additional persistent state variable that affects the distribution of offers.\n",
    "\n",
    "Also notice that this option to leave is distinct from <b>search on the job</b> where a worker samples new wage offers while continuing to work in the existing job, and only accepts an offer if it is better than the current wage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b537429",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "In the McCall (1970) model, the worker understands the probabilistic structure of the model, which allows to form expectations (subjective or objective) over the next-period offers.\n",
    "\n",
    "We now envision a distinctly different algorithm. Imagine that the worker does not have available the probability distribution of next period offers. Instead, the worker observes realized draws and makes accept or reject decisions. The <b>Q-learning algorithm</b> is an example of <b>reinforcement learning algorithms</b> in which the worker is rewarded for making decisions that, over time, lead to high payoffs. Through this process, the worker learns the value of alternative actions in a given state, which then allows to deduce optimal action.\n",
    "\n",
    "We start by rewriting the problem in the form of the <b>state-action value function</b> $Q\\left(\n",
    "w,a\\right) $:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Q\\left( {\\color{#CC6677}{w}},\\text{accept}\\right) &=&{w}+\\beta \\max_{\\left\\{ \\text{accept,\n",
    "reject}\\right\\} }\\left\\{ Q\\left( {w},\\text{accept}\\right) ,Q\\left( {w},\\text{reject}\\right) \\right\\} \\\\\n",
    "Q\\left( {w},\\text{reject}\\right) &=&c+\\beta \\int_{0}^{B}\\max_{\\left\\{ \\text{accept, reject}\\right\\} }\\left\\{ Q\\left( {w^\\prime},\\text{accept}\\right)\n",
    ",Q\\left( {w^\\prime},\\text{reject}\\right) \\right\\} dF\\left( {w^\\prime}\\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\\begin{itemize}\n",
    "\\item notice the distinction between ${w}$ and ${w^\\prime}$\n",
    "\\end{itemize}\n",
    "\n",
    "We now have\n",
    "\n",
    "\\begin{equation*}\n",
    "V^{a}\\left( w\\right) =Q\\left( w,\\text{accept}\\right) \\qquad V\\left( w\\right)\n",
    "=\\max_{\\left\\{ \\text{accept, reject}\\right\\} }\\left\\{ Q\\left( w,\\text{accept}%\n",
    "\\right) ,Q\\left( w,\\text{reject}\\right) \\right\\} .\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542414f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Theoretical exposition\n",
    "\n",
    "\n",
    "\n",
    "% ==============================================================================\n",
    "\n",
    "Temporal difference step\n",
    "\n",
    "We can now replace the expectations with sample draws and form temporal\n",
    "differences.\n",
    "\n",
    "Start with an old iteration of the state-action value function $%\n",
    "Q^{old}\\left( w,a\\right) $\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "TD\\left( {w},\\text{accept}\\right) &=&{w}+\\beta \\max_{a^{\\prime }\\in \\mathcal{%\n",
    "A}}Q^{old}\\left( {w},a^{\\prime }\\right) -Q^{old}\\left( {w},\\text{accept}%\n",
    "\\right) \\\\\n",
    "TD\\left( {w},\\text{reject}\\right) &=&c+\\beta \\max_{a^{\\prime }\\in \\mathcal{A}%\n",
    "}Q^{old}\\left( {w^\\prime},a\\right) -Q^{old}\\left( {w},\\text{reject}\\right)\n",
    "\\end{eqnarray*}%\n",
    "for $\\mathcal{A}=\\left\\{ \\text{accept, reject}\\right\\} $ and ${w^\\prime}\\sim\n",
    "F\\left( {w^\\prime}\\right) $.\n",
    "\n",
    "Then the new iteration of the state-action value function is%\n",
    "\\begin{equation*}\n",
    "Q^{new}\\left( w,a\\right) =Q^{old}\\left( w,a\\right) +\\alpha TD\\left(\n",
    "w,a\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "% ==============================================================================\n",
    "\n",
    "Implementation\n",
    "\n",
    "{Discretization}\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item action is already discrete, replace the state space $\\left[ 0,B\\right]\n",
    "$ with a discrete grid $w^{i}$, $i=1,\\ldots ,I$\n",
    "\n",
    "\\item replace continuous distribution $F\\left( w\\right) $ with a discrete\n",
    "counterpart $\\hat{f}^{i}$ of mass points on grid $w^{i}$\n",
    "\n",
    "\\item sample offers $w^{\\prime }$ from $\\hat{f}^{i}$, $i=1,\\ldots ,I$\n",
    "\n",
    "\\item alternatively, use a form of projection and a `deep Q-learning'\n",
    "algorithm to update the projection coefficients\n",
    "\\end{itemize}\n",
    "\n",
    "{Experimentation}: modify the algorithm to incorporate deviations from\n",
    "currently `optimal' choice\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item in every step, with probability $\\varepsilon $, replace $%\n",
    "\\max_{a^{\\prime }\\in \\mathcal{A}}$ with $\\min_{a^{\\prime }\\in \\mathcal{A}}$\n",
    "\n",
    "\\item allow exploration of alternatives that may be omitted if algorithm\n",
    "gets stuck in a local maximum\n",
    "\\end{itemize}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba4a46",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e534ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done everything.\n"
     ]
    }
   ],
   "source": [
    "# import some useful predefined functions from the course package\n",
    "from course import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c95d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
